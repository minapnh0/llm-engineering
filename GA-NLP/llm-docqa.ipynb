{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "194adab2",
      "metadata": {},
      "source": [
        "#**Business Context**\n",
        "\n",
        "Innovative Research Labs focuses on advancing LLM-based technologies, where quick access to accurate and up-to-date information is essential for driving innovation. As the field evolves rapidly, researchers must efficiently understand complex concepts to maintain a competitive edge.\n",
        "\n",
        "However, the growing volume of technical articles and publications makes it time-consuming to extract relevant insights, especially for foundational topics like the Transformer architecture. This case study demonstrates how a document question-answering system powered by Generative AI can efficiently retrieve and summarize key insights from Jay Alammar’s “The Illustrated Transformer”, streamlining research and improving knowledge discovery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "753fdbf1",
      "metadata": {},
      "source": [
        "### Environment setup\n",
        "Run the cell below first to load API keys. Put a `.env` file in the project root with `HUGGINGFACEHUB_API_TOKEN=your_token`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a51d0e1e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "03b367e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from IPython.display import Markdown, display\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline    \n",
        "from langchain_huggingface import HuggingFacePipeline      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544e1869",
      "metadata": {},
      "source": [
        "LangChain Pipeline Overview\n",
        "\n",
        "The process of transforming unstructured data into a question-answering (QA) system follows these key steps:\n",
        "\n",
        "Data Loading: Raw content is ingested from various sources using LangChain loaders, which convert the data into standardized Document objects.\n",
        "\n",
        "Text Splitting: Documents are divided into smaller, manageable chunks to improve processing and retrieval efficiency.\n",
        "\n",
        "Storage: These chunks are stored—typically in a vector database—where they are embedded for semantic search.\n",
        "\n",
        "Retrieval: Relevant chunks are retrieved from storage based on their similarity to the user’s query.\n",
        "\n",
        "Response Generation: An LLM generates an answer using the user’s question along with the retrieved context.\n",
        "\n",
        "Conversation (Optional): Memory can be added to enable multi-turn interactions and contextual continuity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e9de76e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_repo = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb88e80",
      "metadata": {},
      "source": [
        "## **Step 1: Loading**\n",
        "Use a DocumentLoader to turn unstructured data into Documents.\n",
        "A Document contains the text and its metadata.\n",
        "A WebBaseLoader loads text from HTML and converts it into Documents for NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "97569129",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\"http://jalammar.github.io/illustrated-transformer/\")\n",
        "data = loader.load()  # Loads the content of the webpage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db8d104",
      "metadata": {},
      "source": [
        "## **Step 2: Splitting** \n",
        "\n",
        "Divide the document into smaller segments so they can be embedded and stored efficiently in a vector database.\n",
        "\n",
        "Vector Store:\n",
        "A vector store is a system designed to manage and search unstructured data using embeddings. The typical workflow involves converting documents into embedding vectors and storing them. When a user submits a query, the query is also converted into an embedding. The system then retrieves the stored vectors that are most similar to the query vector. The vector store handles both the storage of embeddings and the similarity search process.\n",
        "\n",
        "Text Embedding:\n",
        "Text embedding is the process of transforming textual data into numerical form, usually as high-dimensional vectors. Each word or token is mapped to a vector in a way that preserves semantic meaning. Words or phrases with similar meanings produce similar vector representations. These embeddings allow machine learning models to interpret and process text by capturing its contextual and semantic relationships in numerical space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "c07a6234",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Split the document into chunks of 2000 characters with 500 characters overlap\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
        "# Noha  changed the chunk_size and chunk_overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1515, chunk_overlap = 100)\n",
        "all_splits = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb824a2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
