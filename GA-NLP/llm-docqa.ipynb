{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fed470fb",
      "metadata": {},
      "source": [
        "#**Business Context**\n",
        "\n",
        "Innovative Research Labs focuses on advancing LLM-based technologies, where quick access to accurate and up-to-date information is essential for driving innovation. As the field evolves rapidly, researchers must efficiently understand complex concepts to maintain a competitive edge.\n",
        "\n",
        "However, the growing volume of technical articles and publications makes it time-consuming to extract relevant insights, especially for foundational topics like the Transformer architecture. This case study demonstrates how a document question-answering system powered by Generative AI can efficiently retrieve and summarize key insights from Jay Alammar’s “The Illustrated Transformer”, streamlining research and improving knowledge discovery."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2db30f5",
      "metadata": {},
      "source": [
        "### Environment setup\n",
        "Run the cell below first to load API keys. Put a `.env` file in the project root with `HUGGINGFACEHUB_API_TOKEN=your_token`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "2cba25c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "fb36a0b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from IPython.display import Markdown, display\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline    \n",
        "from langchain_huggingface import HuggingFacePipeline      \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5f05de6",
      "metadata": {},
      "source": [
        "LangChain Pipeline Overview\n",
        "\n",
        "The process of transforming unstructured data into a question-answering (QA) system follows these key steps:\n",
        "\n",
        "Data Loading: Raw content is ingested from various sources using LangChain loaders, which convert the data into standardized Document objects.\n",
        "\n",
        "Text Splitting: Documents are divided into smaller, manageable chunks to improve processing and retrieval efficiency.\n",
        "\n",
        "Storage: These chunks are stored—typically in a vector database—where they are embedded for semantic search.\n",
        "\n",
        "Retrieval: Relevant chunks are retrieved from storage based on their similarity to the user’s query.\n",
        "\n",
        "Response Generation: An LLM generates an answer using the user’s question along with the retrieved context.\n",
        "\n",
        "Conversation (Optional): Memory can be added to enable multi-turn interactions and contextual continuity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "72fe0f72",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_repo = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "078fbd9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "# Create a transformers pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "# Wrap it in LangChain’s pipeline LLM\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
